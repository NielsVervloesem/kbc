{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch training data\n",
    "\n",
    "```\n",
    "my_training_classes\n",
    "├── person\n",
    "│   ├── han.jpg\n",
    "│   ├── leia.jpg\n",
    "|   ├── luke.jpg\n",
    "│   └── . . .\n",
    "└── ship\n",
    "│   ├── millenium_falcon.jpg\n",
    "│   ├── tie-fighter.jpg    \n",
    "│   ├── x-wing.jpg\n",
    "│   ├── . . .\n",
    "└── . . .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Bucket Name\n",
    "data_bucket_name='deeplens-sagemaker-kbc'\n",
    "\n",
    "# prefix name inside the S3 bucket containing train_rec \n",
    "dataset_name = 'original' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the im2rec.py script on this system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_DIR=/tmp\n",
      "env: S3_DATA_BUCKET_NAME=deeplens-sagemaker-kbc\n",
      "env: DATASET_NAME=original\n",
      "env: IM2REC=/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/tools/im2rec.py\n"
     ]
    }
   ],
   "source": [
    "# Find im2rec in our environment and set up some other vars in our environemnt\n",
    "\n",
    "base_dir='/tmp'\n",
    "\n",
    "%env BASE_DIR=$base_dir\n",
    "%env S3_DATA_BUCKET_NAME = $data_bucket_name\n",
    "%env DATASET_NAME = $dataset_name\n",
    "\n",
    "import sys,os\n",
    "\n",
    "suffix='/mxnet/tools/im2rec.py'\n",
    "im2rec = list(filter( (lambda x: os.path.isfile(x + suffix )), sys.path))[0] + suffix\n",
    "%env IM2REC=$im2rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our training images from S3\n",
    "In order to create training and validation RecordIO files, we need to download our images to our local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull our images from S3\n",
    "!aws s3 sync s3://$S3_DATA_BUCKET_NAME/public/$DATASET_NAME $BASE_DIR/$DATASET_NAME --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RecordIO files from training images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LST files\n",
      "Label classes:\n",
      "bord 0\n",
      "cup 1\n",
      "deep 2\n",
      "empty_bord 3\n",
      "nespresso 4\n",
      "nothing 5\n",
      "oreo 6\n",
      "shure 7\n",
      "small_tr 8\n",
      "yellow 9\n",
      "Creating RecordIO files\n",
      "Creating .rec file from /tmp/original_train.lst in /tmp\n",
      "time: 0.033582210540771484  count: 0\n",
      "time: 161.09551882743835  count: 1000\n",
      "Creating .rec file from /tmp/original_test.lst in /tmp\n",
      "time: 2.240488290786743  count: 0\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 472M Dec 11 12:42 original_test.rec\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 1.2G Dec 11 12:41 original_train.rec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use the IM2REC script to convert our images into RecordIO files\n",
    "\n",
    "# Clean up our working dir of existing LST and REC files\n",
    "cd $BASE_DIR\n",
    "rm *.rec\n",
    "rm *.lst\n",
    "\n",
    "# First we need to create two LST files (training and test lists), noting the correct label class for each image\n",
    "# We'll also save the output of the LST files command, since it includes a list of all of our label classes\n",
    "echo \"Creating LST files\"\n",
    "python $IM2REC --list --recursive --pass-through --test-ratio=0.3 --train-ratio=0.7 $DATASET_NAME $DATASET_NAME > ${DATASET_NAME}_classes\n",
    "\n",
    "echo \"Label classes:\"\n",
    "cat ${DATASET_NAME}_classes\n",
    "\n",
    "# Then we create RecordIO files from the LST files\n",
    "echo \"Creating RecordIO files\"\n",
    "python $IM2REC --num-thread=4 ${DATASET_NAME}_train.lst $DATASET_NAME\n",
    "python $IM2REC --num-thread=4 ${DATASET_NAME}_test.lst $DATASET_NAME\n",
    "ls -lh *.rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload our training and test data RecordIO files so we can train with them\n",
    "Now that we have our training and test .rec files, we upload them to S3 so SageMaker can use them for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../../../tmp/original_train.rec to s3://deeplens-sagemaker-kbc/original/train/original_train.rec\n",
      "upload: ../../../tmp/original_test.rec to s3://deeplens-sagemaker-kbc/original/validation/original_test.rec\n"
     ]
    }
   ],
   "source": [
    "# Upload our train and test RecordIO files to S3 in the bucket that our sagemaker session is using\n",
    "bucket = 'deeplens-sagemaker-kbc'\n",
    "\n",
    "s3train_path = 's3://{}/{}/train/'.format(bucket, dataset_name)\n",
    "s3validation_path = 's3://{}/{}/validation/'.format(bucket, dataset_name)\n",
    "\n",
    "# Clean up any existing data\n",
    "!aws s3 rm s3://{bucket}/{dataset_name}/train --recursive\n",
    "!aws s3 rm s3://{bucket}/{dataset_name}/validation --recursive\n",
    "\n",
    "# Upload the rec files to the train and validation channels\n",
    "!aws s3 cp /tmp/{dataset_name}_train.rec $s3train_path\n",
    "!aws s3 cp /tmp/{dataset_name}_test.rec $s3validation_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the data for our model training to use\n",
    "Finally, we tell SageMaker where to find these RecordIO files to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(\n",
    "    s3train_path, \n",
    "    distribution='FullyReplicated', \n",
    "    content_type='application/x-recordio', \n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(\n",
    "    s3validation_path, \n",
    "    distribution='FullyReplicated', \n",
    "    content_type='application/x-recordio', \n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now it's time to train our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an image classifier object with some base configuration\n",
    "More info here: https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, dataset_name)\n",
    "\n",
    "image_classifier = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role, \n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some training hyperparameters\n",
    "\n",
    "Finally, before we train, we provide some additional configuration parameters for the training.\n",
    "\n",
    "More info here: https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_pretrained_model': 1,\n",
       " 'image_shape': '3,224,224',\n",
       " 'num_classes': 10,\n",
       " 'num_training_samples': 1061,\n",
       " 'epochs': 2,\n",
       " 'learning_rate': 0.001,\n",
       " 'mini_batch_size': 5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes=! ls -l {base_dir}/{dataset_name} | wc -l\n",
    "num_classes=int(num_classes[0]) - 1\n",
    "\n",
    "num_training_samples=! cat {base_dir}/{dataset_name}_train.lst | wc -l\n",
    "num_training_samples = int(num_training_samples[0])\n",
    "\n",
    "# Learn more about the Sagemaker built-in Image Classifier hyperparameters here: https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html\n",
    "\n",
    "# These hyperparameters we won't want to change, as they define things like\n",
    "# the size of the images we'll be sending for input, the number of training classes we have, etc.\n",
    "base_hyperparameters=dict(\n",
    "    use_pretrained_model=1,\n",
    "    image_shape='3,224,224',\n",
    "    num_classes=num_classes,\n",
    "    num_training_samples=num_training_samples,\n",
    "    epochs = 2\n",
    ")\n",
    "\n",
    "# These are hyperparameters we may want to tune, as they can affect the model training success:\n",
    "hyperparameters={\n",
    "    **base_hyperparameters, \n",
    "    **dict(\n",
    "        learning_rate=0.001,\n",
    "        mini_batch_size=5,\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "image_classifier.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    augmentation_type=\"crop_color_transform\",\n",
    "    beta_1 = 0.9, #used for adam optimizer\n",
    "    beta_2 = 0.999, #used for adam optimizer\n",
    "    epochs = 5,\n",
    "    image_shape = \"3,224,224\",\n",
    "    learning_rate = 0.1,\n",
    "    lr_scheduler_factor = 0.1,\n",
    "    lr_scheduler_step = 3,\n",
    "    mini_batch_size = 32,\n",
    "    momentum = 0.9,\n",
    "    num_classes = 5, #variable\n",
    "    num_layers = 50, # The algorithm supports multiple network depth (number of layers). They are 18, 34, 50, 101, 152 and 200\\num_training_samples\t10000\n",
    "    num_training_samples = 10000, #variable\n",
    "    optimizer = \"adam\",\n",
    "    precision_dtype = \"float32\",\n",
    "    use_pretrained_model = 1,\n",
    "    weight_decay = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training\n",
    "Train our model!\n",
    "\n",
    "This will take some time because it's provisioning a new container runtime to train our model, then the actual training happens, then the trained model gets uploaded to S3 and the container is shut down.\n",
    "\n",
    "More info here: https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-11 12:43:11 Starting - Starting the training job...\n",
      "2019-12-11 12:43:13 Starting - Launching requested ML instances......\n",
      "2019-12-11 12:44:20 Starting - Preparing the instances for training.........\n",
      "2019-12-11 12:46:11 Downloading - Downloading input data.........\n",
      "2019-12-11 12:47:34 Training - Downloading the training image..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\n",
      "2019-12-11 12:47:54 Training - Training image download completed. Training in progress.\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'use_pretrained_model': u'1', u'epochs': u'2', u'num_training_samples': u'1061', u'image_shape': u'3,224,224', u'mini_batch_size': u'5', u'learning_rate': u'0.001', u'num_classes': u'10'}\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] Final configuration: {u'optimizer': u'sgd', u'learning_rate': u'0.001', u'epochs': u'2', u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'precision_dtype': u'float32', u'mini_batch_size': u'5', u'num_classes': u'10', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': 0, u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'1061'}\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] multi_label: 0\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] num_layers: 152\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] epochs: 2\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] optimizer: sgd\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] momentum: 0.9\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] learning_rate: 0.001\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] num_training_samples: 1061\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] mini_batch_size: 5\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] num_classes: 10\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] kv_store: device\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:47:59 INFO 139625756972864] --------------------\u001b[0m\n",
      "\u001b[34m[12:47:59] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[12:47:59] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:48:01 INFO 139625756972864] Setting number of threads: 3\u001b[0m\n",
      "\u001b[34m[12:48:11] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:48:27 INFO 139625756972864] Epoch[0] Batch [20]#011Speed: 6.245 samples/sec#011accuracy=0.390476\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:48:34 INFO 139625756972864] Epoch[0] Batch [40]#011Speed: 8.628 samples/sec#011accuracy=0.551220\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:48:41 INFO 139625756972864] Epoch[0] Batch [60]#011Speed: 9.875 samples/sec#011accuracy=0.672131\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:48:49 INFO 139625756972864] Epoch[0] Batch [80]#011Speed: 10.622 samples/sec#011accuracy=0.718519\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:48:56 INFO 139625756972864] Epoch[0] Batch [100]#011Speed: 11.136 samples/sec#011accuracy=0.766337\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:49:03 INFO 139625756972864] Epoch[0] Batch [120]#011Speed: 11.502 samples/sec#011accuracy=0.801653\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:49:10 INFO 139625756972864] Epoch[0] Batch [140]#011Speed: 11.778 samples/sec#011accuracy=0.826950\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:49:18 INFO 139625756972864] Epoch[0] Batch [160]#011Speed: 11.991 samples/sec#011accuracy=0.845963\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:49:25 INFO 139625756972864] Epoch[0] Batch [180]#011Speed: 12.164 samples/sec#011accuracy=0.860773\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:49:32 INFO 139625756972864] Epoch[0] Batch [200]#011Speed: 12.303 samples/sec#011accuracy=0.874627\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:49:36 INFO 139625756972864] Epoch[0] Train-accuracy=0.879245\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:49:36 INFO 139625756972864] Epoch[0] Time cost=85.236\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:49:58 INFO 139625756972864] Epoch[0] Validation-accuracy=0.995604\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:49:58 INFO 139625756972864] Storing the best model with validation accuracy: 0.995604\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:49:58 INFO 139625756972864] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:50:06 INFO 139625756972864] Epoch[1] Batch [20]#011Speed: 13.134 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:50:13 INFO 139625756972864] Epoch[1] Batch [40]#011Speed: 13.384 samples/sec#011accuracy=0.995122\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:50:21 INFO 139625756972864] Epoch[1] Batch [60]#011Speed: 13.499 samples/sec#011accuracy=0.993443\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:50:28 INFO 139625756972864] Epoch[1] Batch [80]#011Speed: 13.497 samples/sec#011accuracy=0.990123\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:50:35 INFO 139625756972864] Epoch[1] Batch [100]#011Speed: 13.533 samples/sec#011accuracy=0.988119\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:50:43 INFO 139625756972864] Epoch[1] Batch [120]#011Speed: 13.548 samples/sec#011accuracy=0.988430\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:50:50 INFO 139625756972864] Epoch[1] Batch [140]#011Speed: 13.559 samples/sec#011accuracy=0.990071\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:50:57 INFO 139625756972864] Epoch[1] Batch [160]#011Speed: 13.563 samples/sec#011accuracy=0.988820\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:51:05 INFO 139625756972864] Epoch[1] Batch [180]#011Speed: 13.566 samples/sec#011accuracy=0.987845\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:51:12 INFO 139625756972864] Epoch[1] Batch [200]#011Speed: 13.572 samples/sec#011accuracy=0.988060\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:51:16 INFO 139625756972864] Epoch[1] Train-accuracy=0.985849\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:51:16 INFO 139625756972864] Epoch[1] Time cost=77.661\u001b[0m\n",
      "\n",
      "2019-12-11 12:51:43 Uploading - Uploading generated training model\u001b[34m[12/11/2019 12:51:37 INFO 139625756972864] Epoch[1] Validation-accuracy=0.997802\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:51:37 INFO 139625756972864] Storing the best model with validation accuracy: 0.997802\u001b[0m\n",
      "\u001b[34m[12/11/2019 12:51:38 INFO 139625756972864] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\n",
      "2019-12-11 12:52:20 Completed - Training job completed\n",
      "Training seconds: 369\n",
      "Billable seconds: 369\n",
      "\n",
      "\n",
      " Finished training! The model is available for download at: s3://deeplens-sagemaker-kbc/original/output/IC-original-1576068191/output/model.tar.gz\n",
      "CPU times: user 1.19 s, sys: 0 ns, total: 1.19 s\n",
      "Wall time: 9min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "now = str(int(time.time()))\n",
    "training_job_name = 'IC-' + dataset_name.replace('_', '-') + '-' + now\n",
    "\n",
    "image_classifier.fit(inputs=data_channels, job_name=training_job_name, logs=True)\n",
    "\n",
    "job = image_classifier.latest_training_job\n",
    "model_path = f\"{base_dir}/{job.name}\"\n",
    "\n",
    "print(f\"\\n\\n Finished training! The model is available for download at: {image_claussifier.output_path}/{job.name}/output/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "hyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'adam']),\n",
    "                         'learning_rate': ContinuousParameter(0.0001, 0.1),\n",
    "                         'mini_batch_size': IntegerParameter(2, 32),\n",
    "                        }\n",
    "\n",
    "objective_metric_name = 'validation:accuracy'\n",
    "\n",
    "tuner = HyperparameterTuner(image_classifier,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=50,\n",
    "                            max_parallel_jobs=3)\n",
    "\n",
    "tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
